{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T13:43:14.324666Z",
     "start_time": "2020-01-20T13:43:12.982308Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=3)\n",
    "#np.core.arrayprint._line_width = 500\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.5g\" % x))\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "import pickle\n",
    "import utility\n",
    "import util_predict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T22:52:42.874951Z",
     "start_time": "2019-10-29T22:52:42.862983Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Creating T1\n",
    "_array = np.arange(9).reshape(3,3)\n",
    "input_dims = [5,5]\n",
    "output_dims = [ 15 , 15 ]\n",
    "\n",
    "inp_dim_h = input_dims[0]\n",
    "inp_dim_w = input_dims[1]\n",
    "\n",
    "outp_dim_h = output_dims[0]\n",
    "outp_dim_w = output_dims[1]\n",
    "\n",
    "upsample_h = outp_dim_h - inp_dim_h #amount to expand in height dimension\n",
    "upsample_w = outp_dim_w - inp_dim_w\n",
    "\n",
    "upsample_h_inner = outp_dim_h - (upsample_h % (inp_dim_h-1) ) #amount to expand in height dimension, by inner padding\n",
    "upsample_w_inner = outp_dim_w - (upsample_w % (inp_dim_w-1) ) #amount to expand in width dimension, w/ inner padding\n",
    "\n",
    "stride_h = int( (upsample_h_inner-inp_dim_h)/(inp_dim_h-1) )\n",
    "stride_w = int( (upsample_w_inner-inp_dim_w)/(inp_dim_w-1) )\n",
    "\n",
    "# Creating transformation matrices\n",
    "T_1 = np.zeros( (upsample_h_inner, inp_dim_h) )\n",
    "T_2 = np.zeros( (inp_dim_w, upsample_w_inner ) )\n",
    "\n",
    "d1 = np.einsum('ii->i', T_1[ ::stride_h+1,:] ) \n",
    "d1 += 1\n",
    "\n",
    "d2 = np.einsum('ii->i', T_2[ :, ::stride_w+1] )\n",
    "d2 += 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T22:59:44.265124Z",
     "start_time": "2019-10-29T22:59:44.256148Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "T_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T22:50:26.751215Z",
     "start_time": "2019-10-29T22:50:26.743236Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_image = np.ones((1,5,5,1))\n",
    "_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T22:36:14.747474Z",
     "start_time": "2019-10-29T22:36:14.739500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_temp = np.matmul( T_1, _image)\n",
    "_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T22:52:07.371604Z",
     "start_time": "2019-10-29T22:52:07.360625Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.einsum('ij,jk->ik', T_1, _image[0,:,:,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-29T23:01:22.334960Z",
     "start_time": "2019-10-29T23:01:22.329981Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_images1 = np.einsum('ij,hjkl->hikl', T_1, _image )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T16:04:33.006109Z",
     "start_time": "2019-10-31T16:04:32.995138Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# data pipeline hdr images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:00:14.538432Z",
     "start_time": "2019-11-06T13:00:14.511508Z"
    },
    "code_folding": [
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_num_parallel_calls =tf.data.experimental.AUTOTUNE \n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# region elevation\n",
    "_path = \"Data/Preprocessed/elevation.pkl\" #TODO:(akanni-ade) change to value passed in via a h-parameters dictionary\n",
    "with open(_path,\"rb\") as f: #TODO:(akanni-ade) change to value passed in via a h-parameters dictionary\n",
    "    arr_elev = pickle.load(f)\n",
    "    \n",
    "arr_elev = arr_elev[::4, ::4]  #shape( 156,352 ) #16kmby16km\n",
    "    #creating layered representation of 16kmby16km arr_elev such that it is same shape as 64kmby64km precip\n",
    "        ##take cell i,j in 2d array. each cell in the square matrix around cell i,j is stacked underneath i,j. \n",
    "        ## The square has dimensions (rel to i,j): 2 to the right, 2 down, 1 left, 1 right\n",
    "        ## This achieves a dimension reduction of 4\n",
    "MAX_ELEV = 2500 #TODO: (akanni-ade) Find actual max elev\n",
    "arr_elev = arr_elev / MAX_ELEV \n",
    "\n",
    "def stacked_reshape( arr, first_centre, downscale_x, downscale_y, batch_size = BATCH_SIZE ):\n",
    "    \"\"\"\n",
    "        This produces a list of tiled arrays. This ensures higher resolution data _arr has the same shape as lower resolution data, ignoring a depth dimension\n",
    "        i.e.\n",
    "\n",
    "        The array is stacked to create a new dimension (axis=-1). The stack happens on the following cells:\n",
    "            first centre (i,j)\n",
    "            (i+n*downscale_x, j+n*downscale_y ) for integer n\n",
    "\n",
    "        :param arr: 2D array\n",
    "        :first centre: tuple (i,j) indexing where the upperleft most position to stack on\n",
    "\n",
    "        returns arr_stacked\n",
    "    \"\"\"\n",
    "    new_depth = downscale_x * downscale_y\n",
    "    dim_x, dim_y = arr.shape\n",
    "    li_arr = []\n",
    "\n",
    "    idxs_x = list( range(downscale_x) )\n",
    "    idxs_y = list( range(downscale_y) )\n",
    "    starting_idxs = list( itertools.product( idxs_x, idxs_y ) )\n",
    "\n",
    "    for x,y in starting_idxs:\n",
    "        end_x = dim_x - ( downscale_x-first_centre[0] - x) \n",
    "        end_y = dim_y - ( downscale_y-first_centre[1] - y)\n",
    "        arr_cropped = arr[ x:end_x, y:end_y ]\n",
    "\n",
    "        li_arr.append(arr_cropped)\n",
    "\n",
    "    li_tnsr = [ tf.expand_dims(_arr[::downscale_x, ::downscale_y],0) for _arr in li_arr ]\n",
    "    li_tnsr_elev =  [ tf.tile(_tnsr,[BATCH_SIZE,1,1]) for _tnsr in li_tnsr ]\n",
    "    tnsr_elev_tiled = tf.stack(li_tnsr_elev, axis=-1)\n",
    "    \n",
    "    #arr_stacked = np.stack( li_arr, axis=-1 )\n",
    "    #arr_stacked = arr_stacked[::downscale_x, ::downscale_y] \n",
    "\n",
    "    return tnsr_elev_tiled\n",
    "\n",
    "\n",
    "tnsr_elev_tiled = stacked_reshape( arr_elev, (1,1), 4, 4  ) # list[ (1, 39, 88), (1, 39, 88), ... ] #16kmby16km "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tnsr_elev_tiled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:00:25.946308Z",
     "start_time": "2019-11-06T13:00:25.392467Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# region precip, features, targets\n",
    "_num_parallel_calls=1\n",
    "BATCH_SIZE=1\n",
    "_dir_precip = \"./Data/PRISM/daily_precip\"\n",
    "file_paths_bil = list( glob.glob(_dir_precip+\"/*/*.bil\" ) )\n",
    "file_paths_bil.sort(reverse=False)\n",
    "\n",
    "ds_fns_precip = tf.data.Dataset.from_tensor_slices(file_paths_bil)\n",
    "\n",
    "ds_precip_imgs = ds_fns_precip.map( lambda fn: tf.py_function(utility.read_prism_precip,[fn], [tf.float32] ), num_parallel_calls=_num_parallel_calls ) #shape(bs, 621, 1405) #4km by 4km\n",
    "\n",
    "ds_precip_imgs = ds_precip_imgs.batch(BATCH_SIZE,drop_remainder=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T11:49:14.478418Z",
     "start_time": "2019-11-06T11:49:14.422572Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "next(iter(ds_precip_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:00:33.859083Z",
     "start_time": "2019-11-06T13:00:33.846123Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def features_labels_mker( arr_images, tnsr_elev_tiled=tnsr_elev_tiled ):\n",
    "    \"\"\"Produces the precipitation features and the target for training\n",
    "    shape(bs, rows, cols)\n",
    "    \"\"\"\n",
    "    #standardisation\n",
    "    MAX_RAIN = 200 #TODO:(akanni-ade) Find actual max rain\n",
    "    arr_images = arr_images / MAX_RAIN \n",
    "\n",
    "    #features\n",
    "    precip_feat = reduce_res( arr_images, 16, 16 ) #shape(bs, 621/16, 1405/16) (bs, 39, 88)  64km by 64km\n",
    "\n",
    "\n",
    "    feat = tf.concat( [tf.expand_dims(precip_feat,-1),tnsr_elev_tiled], axis=-1 ) #shape(bs, 39, 88, 17)  64km by 64km\n",
    "\n",
    "    #targets        \n",
    "    precip_tar = reduce_res( arr_images, 4, 4)   #shape( bs, 621/4, 1405/4 ) (bs,156,352) 16km by 16km\n",
    "\n",
    "    #TODO(akanni-ade): consider applying cropping to remove large parts that are just water \n",
    "        # #cropping\n",
    "        # precip_tar = precip_tar[:, : , : ]\n",
    "        # feat = feat[:, :, :, :]\n",
    "\n",
    "    return feat, precip_tar\n",
    "\n",
    "def reduce_res(arr_imgs, x_axis, y_axis):\n",
    "    arr_imgs_red = arr_imgs[:,::x_axis, ::y_axis]\n",
    "    return arr_imgs_red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:00:42.151451Z",
     "start_time": "2019-11-06T13:00:41.900057Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds_precip_feat_tar = ds_precip_imgs.map( features_labels_mker, num_parallel_calls=_num_parallel_calls ) #shape( (bs, 39, 88, 17 ) (bs,156,352) )\n",
    "ds_precip_feat_tar = ds_precip_feat_tar\n",
    "\n",
    "# endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:00:44.454584Z",
     "start_time": "2019-11-06T13:00:44.295762Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "feat_tar = next(iter(ds_precip_feat_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T13:00:49.128702Z",
     "start_time": "2019-11-06T13:00:49.109758Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Creating Boolean mask to ignore the water Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bool_mask = np.logical_not( np.isnan(tf.squeeze(feat_tar[1]) ) )  #True means it should not be masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'_'.join( map( str, bool_mask.shape ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(bool_mask,open(\"Images/water_mask_{}.dat\".format('_'.join( map( str, bool_mask.shape ) )),\"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Sampling from distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T15:53:00.080275Z",
     "start_time": "2019-11-06T15:53:00.072300Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_loc = [1,2,3,4,5,6]\n",
    "_scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T15:53:01.172675Z",
     "start_time": "2019-11-06T15:53:01.163706Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dist = tfp.distributions.Normal(loc=_loc, scale=_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T15:53:02.448200Z",
     "start_time": "2019-11-06T15:53:02.436233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dist.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T15:53:05.741807Z",
     "start_time": "2019-11-06T15:53:05.711884Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = [1,2,3,4,5,6]\n",
    "dist.log_prob( [1,2,3,4,5,6] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# LinearOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T22:37:21.639557Z",
     "start_time": "2019-11-11T22:37:21.610606Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(12.0)\n",
    "a = tf.random.uniform((3,3)) + b\n",
    "a_lin = tf.linalg.LinearOperatorFullMatrix([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T22:37:21.660475Z",
     "start_time": "2019-11-11T22:37:21.644516Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a_lin.to_dense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T22:38:08.300169Z",
     "start_time": "2019-11-11T22:38:08.283218Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.split(a_lin.to_dense(),3 ,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T20:38:42.271232Z",
     "start_time": "2019-11-10T20:38:42.256280Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c = tf.linalg.LinearOperatorDiag([1,2,3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T20:44:25.432204Z",
     "start_time": "2019-11-10T20:44:25.413253Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d = tf.ones_like(a)\n",
    "e  = a_lin.add_to_tensor(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T20:44:28.955245Z",
     "start_time": "2019-11-10T20:44:28.941285Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-10T20:52:16.111453Z",
     "start_time": "2019-11-10T20:52:16.096493Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f = tf.linalg.LinearOperatorKronecker([a_lin,c]).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-10T20:55:13.633Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfd.MultivariateNormalFullCovariance(loc=0 , covariance_matrix=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T02:13:39.664723Z",
     "start_time": "2019-11-07T02:13:39.640753Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = tf.random.normal(tf.reshape(4750, [-1]),1,1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T02:09:40.751092Z",
     "start_time": "2019-11-07T02:09:40.707149Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.Variable(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T02:12:13.628526Z",
     "start_time": "2019-11-07T02:12:13.600523Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tf.random.normal( tf.reshape(tf.constant(4500),[-1]), mean=0, stddev=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.linspace(88,352,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:17:04.163261Z",
     "start_time": "2020-01-08T18:17:04.153623Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# checking quantiles of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:19:34.110589Z",
     "start_time": "2020-01-08T18:19:34.105822Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_path_pred_eval = _path_pred = \"./Output/{}/{}/EvaluatedPredictions\".format(\"DeepSD\", 3)\n",
    "gen_data_eval_preds = util_predict.load_predictions_gen(_path_pred_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:20:09.695790Z",
     "start_time": "2020-01-08T18:20:09.537513Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "datum = next(gen_data_eval_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:20:54.096523Z",
     "start_time": "2020-01-08T18:20:54.091956Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "(np_rmse, np_bias, np_lower_bands, np_upper_bands, true_in_pred_range) = datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:29:34.905569Z",
     "start_time": "2020-01-08T18:29:34.902388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T18:32:42.495150Z",
     "start_time": "2020-01-08T18:32:42.490909Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx=2\n",
    "x_s, x_e = 100, 120\n",
    "y_s, y_e = 200, 220\n",
    "print(np_lower_bands[idx,x_s:x_e,y_s:y_e])\n",
    "print(\"\\n\",np_upper_bands[idx,x_s:x_e,y_s:y_e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T22:09:03.977772Z",
     "start_time": "2020-01-08T22:09:03.840575Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import hparameters\n",
    "\n",
    "s_dir = utility.get_script_directory(sys.argv[0])\n",
    "\n",
    "#args_dict = utility.parse_arguments(s_dir)\n",
    "\n",
    "test_params = hparameters.test_hparameters(**{'script_dir':'/home/u1862646/ATI/BNN'})\n",
    "\n",
    "#stacked DeepSd methodology\n",
    "li_input_output_dims = [ {\"input_dims\": [39, 88 ], \"output_dims\": [98, 220 ] , 'var_model_type':'guassian_factorized' } ,\n",
    "             {\"input_dims\": [98, 220 ] , \"output_dims\": [ 156, 352 ] , 'conv1_inp_channels':1, 'var_model_type':'guassian_factorized' }  ]\n",
    "\n",
    "model_params = [ hparameters.model_deepsd_hparameters(**_dict) for _dict in li_input_output_dims  ]\n",
    "model_params = [ mp() for mp in model_params]\n",
    "\n",
    "model, checkpoint_code = util_predict.load_model(test_params(), model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T22:10:07.590749Z",
     "start_time": "2020-01-08T22:10:07.587495Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names = []\n",
    "qmeans = []\n",
    "qstds = []\n",
    "for i, layer in enumerate(model.layers):\n",
    "    try:\n",
    "        q = layer.kernel_posterior\n",
    "    except AttributeError:\n",
    "        continue\n",
    "    names.append(\"Layer {}\".format(i))\n",
    "    qmeans.append(q.mean())\n",
    "    qstds.append(q.stddev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T22:10:48.715002Z",
     "start_time": "2020-01-08T22:10:48.713001Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-08T22:14:39.527342Z",
     "start_time": "2020-01-08T22:14:39.524087Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T02:02:49.897449Z",
     "start_time": "2020-01-20T02:02:49.883326Z"
    }
   },
   "source": [
    "# Checing Data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:34:53.081999Z",
     "start_time": "2020-01-20T14:34:53.077704Z"
    }
   },
   "outputs": [],
   "source": [
    "fn  = \"/home/u1862646/ATI/BNN/Data/Rain_Data_Nov19/rr_ens_mean_0.1deg_reg_v20.0e_197901-201907_uk.nc\" \n",
    "import data_generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T15:18:35.935281Z",
     "start_time": "2020-01-20T15:18:35.002756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 0.00000e+00,\n",
       "         0.00000e+00, 0.00000e+00],\n",
       "        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 0.00000e+00,\n",
       "         0.00000e+00, 0.00000e+00],\n",
       "        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 0.00000e+00,\n",
       "         0.00000e+00, 0.00000e+00],\n",
       "        ...,\n",
       "        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n",
       "         9.96921e+36, 9.96921e+36],\n",
       "        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n",
       "         9.96921e+36, 9.96921e+36],\n",
       "        [9.96921e+36, 9.96921e+36, 9.96921e+36, ..., 9.96921e+36,\n",
       "         9.96921e+36, 9.96921e+36]], dtype=float32),\n",
       " array([[ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        [ True,  True,  True, ..., False, False, False],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#rr_ens_mean_0.1deg_reg_v20.0e_197901-201907_uk.nc\"\n",
    "gen = data_generators.Generator_rain(fn=fn, all_at_once=False)(0)#(0)\n",
    "\n",
    "next(iter(gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:36:46.382551Z",
     "start_time": "2020-01-20T14:36:46.358770Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_tar = tf.data.Dataset.from_generator(lambda: data_generators.Generator_rain(fn=fn, all_at_once=True)(), output_types=( tf.float32, tf.bool) ) #(values, mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:36:49.682286Z",
     "start_time": "2020-01-20T14:36:48.270899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=125, shape=(14822, 100, 140), dtype=float32, numpy=\n",
       " array([[[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          1.1000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          1.1000000e+00, 1.1000000e+00, 1.1000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          1.1000000e+00, 9.0000004e-01, 9.0000004e-01],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]]], dtype=float32)>,\n",
       " <tf.Tensor: id=126, shape=(14822, 100, 140), dtype=bool, numpy=\n",
       " array([[[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]])>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:34:32.281324Z",
     "start_time": "2020-01-20T14:34:32.270994Z"
    },
    "code_folding": [
     32,
     56,
     57,
     105
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4\n",
    "from netCDF4 import Dataset, num2date\n",
    "import gdal\n",
    "import pupygrib\n",
    "import tensorflow as tf\n",
    "#TODO:(akanni-ade): add ability to return long/lat variable  implement long/lat\n",
    "\n",
    "\"\"\"\n",
    "    Example of how to use\n",
    "    import Generators\n",
    "\n",
    "    rr_ens file \n",
    "    _filename = \"Data/Rain_Data/rr_ens_mean_0.1deg_reg_v20.0e_197901-201907_djf_uk.nc\"\n",
    "    rain_gen = Generator_rain(_filename, all_at_once=True)\n",
    "    data = next(iter(grib_gen))\n",
    "\n",
    "    Grib Files\n",
    "    _filename = 'Data/Rain_Data/ana_coarse.grib'\n",
    "    grib_gen = Generators.Generator_grib(fn=_filename, all_at_once=True)\n",
    "    data = next(iter(grib_gen))\n",
    "\n",
    "    Grib Files Location:\n",
    "    _filename = 'Data/Rain_Data/ana_coarse.grib'\n",
    "    grib_gen = Generators.Generator_grib(fn=_filename, all_at_once=True)\n",
    "    arr_long, arr_lat = grib_gen.locaiton()\n",
    "    #now say you are investingating the datum x = data[15,125]\n",
    "    #   to get the longitude and latitude you must do\n",
    "    long, lat = arr_long(15,125), arr_lat(15,125)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class Generator():\n",
    "    \n",
    "    def __init__(self, fn = \"\", all_at_once=False, train_size=0.75, channel=None ):\n",
    "        self.generator = None\n",
    "        self.all_at_once = all_at_once\n",
    "        self.fn = fn\n",
    "        self.channel = channel\n",
    "    \n",
    "    def yield_all(self):\n",
    "        pass\n",
    "\n",
    "    def yield_iter(self):\n",
    "        pass\n",
    "\n",
    "    def long_lat(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self,x=0):\n",
    "        if(self.all_at_once):\n",
    "            return self.yield_all()\n",
    "        else:\n",
    "            return self.yield_iter(x)\n",
    "    \n",
    "\n",
    "class Generator_mf(Generator):\n",
    "    \"\"\"\n",
    "        Creates a generator for the model_fields_data\n",
    "    \n",
    "        :param all_at_once: whether to return all data, or return data in batches\n",
    "\n",
    "        :param channel: the desired channel of information in the grib file\n",
    "            Default None, then concatenate all channels together and return\n",
    "            If an integer return this band\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **generator_params):\n",
    "\n",
    "        super(Generator_mf, self).__init__(**generator_params)\n",
    "\n",
    "        self.vars_for_feature = ['unknown_local_param_137_128', 'unknown_local_param_133_128', 'air_temperature', 'geopotential', 'x_wind', 'y_wind' ]\n",
    "        self.channel_count = len(self.vars_for_feature) \n",
    "        \n",
    "\n",
    "    def yield_all(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def yield_iter(self):\n",
    "        with Dataset(self.fn, \"r\", format=\"NETCDF4\") as f:\n",
    "            for tuple_mfs in zip( *[f.variables[var_name] for var_name in self.vars_for_feature ] ):\n",
    "                \n",
    "                list_datamask = [ (np.ma.getdata(_mar),np.ma.getmask(_mar) ) for _mar in tuple_mfs ]\n",
    "                \n",
    "                _data, _masks= list( zip (*list_datamask ) )\n",
    "                \n",
    "                \n",
    "                stacked_data = np.stack(_data, axis=-1 )\n",
    "                stacked_masks = np.stack(_masks, axis=-1 )\n",
    "                \n",
    "                yield stacked_data, stacked_masks\n",
    "            \n",
    "            \n",
    "        \n",
    "    def location(self):\n",
    "        \"\"\"\n",
    "        Returns a 2 1D arrays\n",
    "            arr_long: Longitudes\n",
    "            arr_lat: Latitdues\n",
    "        Example of how to use:\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "class Generator_rain2(Generator):\n",
    "    def __init__(self, **generator_params):\n",
    "        super(Generator_rain2, self).__init__(**generator_params)\n",
    "\n",
    "    def yield_all(self,start_idx=0):\n",
    "        with Dataset(self.fn, \"r\", format=\"NETCDF4\") as f:\n",
    "            _data = f.variables['rr'][start_idx:]\n",
    "            yield np.ma.getdata(_data), np.ma.getmask(_data)   \n",
    "            \n",
    "    def yield_iter(self,start_idx=0):\n",
    "        f = Dataset(self.fn, \"r\", format=\"NETCDF4\")\n",
    "        #with Dataset(self.fn, \"r\", format=\"NETCDF4\") as f:\n",
    "            #for chunk in f.variables['rr'][start_idx:]:\n",
    "        \n",
    "        #for chunk in f.variables['rr'][start_idx:]:\n",
    "        for chunk in f.variables['rr'][start_idx:]:\n",
    "            yield np.ma.getdata(chunk), np.ma.getmask(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:34:35.948291Z",
     "start_time": "2020-01-20T14:34:34.309065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=61, shape=(14822, 100, 140), dtype=float32, numpy=\n",
       " array([[[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          1.1000000e+00, 1.0000000e+00, 1.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          1.1000000e+00, 1.1000000e+00, 1.1000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          1.1000000e+00, 9.0000004e-01, 9.0000004e-01],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]],\n",
       " \n",
       "        [[9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "         ...,\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36],\n",
       "         [9.9692100e+36, 9.9692100e+36, 9.9692100e+36, ...,\n",
       "          9.9692100e+36, 9.9692100e+36, 9.9692100e+36]]], dtype=float32)>,\n",
       " <tf.Tensor: id=62, shape=(14822, 100, 140), dtype=bool, numpy=\n",
       " array([[[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]],\n",
       " \n",
       "        [[ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         [ True,  True,  True, ..., False, False, False],\n",
       "         ...,\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True],\n",
       "         [ True,  True,  True, ...,  True,  True,  True]]])>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tar = tf.data.Dataset.from_generator(lambda: Generator_rain2(fn=fn, all_at_once=True)(), output_types=( tf.float32, tf.bool) ) #(values, mask) \n",
    "next(iter(ds_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
